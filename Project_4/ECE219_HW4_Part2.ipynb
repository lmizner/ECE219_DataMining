{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d202446-a009-4d5c-81cb-8999a1296757",
   "metadata": {},
   "source": [
    "# Project 4: Regression Analysis and Define Your Own Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fba740a-b696-4b9c-a7c2-df6a6dfd6527",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62787176-a56a-40b4-98bd-85b76b970489",
   "metadata": {},
   "source": [
    "### Question 9.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c060d38b-2e7a-42a5-8cf3-26ff906648ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECE219_tweet_data/tweets_#gohawks.txt\n",
      "Average number of tweets per hour: 292.48785062173687\n",
      "Average number of followers of users posting the tweets per tweet: 2217.9237355281984\n",
      "Average number of retweets per tweet: 2.0132093991319877\n",
      "--------------------------------------------------\n",
      "ECE219_tweet_data/tweets_#gopatriots.txt\n",
      "Average number of tweets per hour: 40.954698006061946\n",
      "Average number of followers of users posting the tweets per tweet: 1427.2526051635405\n",
      "Average number of retweets per tweet: 1.4081919101697078\n",
      "--------------------------------------------------\n",
      "ECE219_tweet_data/tweets_#nfl.txt\n",
      "Average number of tweets per hour: 397.0213901819841\n",
      "Average number of followers of users posting the tweets per tweet: 4662.37544523693\n",
      "Average number of retweets per tweet: 1.5344602655543254\n",
      "--------------------------------------------------\n",
      "ECE219_tweet_data/tweets_#patriots.txt\n",
      "Average number of tweets per hour: 750.8942646068899\n",
      "Average number of followers of users posting the tweets per tweet: 3280.4635616550277\n",
      "Average number of retweets per tweet: 1.7852871288476946\n",
      "--------------------------------------------------\n",
      "ECE219_tweet_data/tweets_#sb49.txt\n",
      "Average number of tweets per hour: 1276.8570598680474\n",
      "Average number of followers of users posting the tweets per tweet: 10374.160292019487\n",
      "Average number of retweets per tweet: 2.52713444111402\n",
      "--------------------------------------------------\n",
      "ECE219_tweet_data/tweets_#superbowl.txt\n",
      "Average number of tweets per hour: 2072.11840170408\n",
      "Average number of followers of users posting the tweets per tweet: 8814.96799424623\n",
      "Average number of retweets per tweet: 2.3911895819207736\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def calculate_statistics(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        tweets = [json.loads(line) for line in file]\n",
    "    \n",
    "    num_tweets = len(tweets)\n",
    "    if num_tweets == 0:\n",
    "        print(f\"No tweets found in {filename}\")\n",
    "        return\n",
    "    \n",
    "    times = [tweet['citation_date'] for tweet in tweets]\n",
    "    max_time = max(times)\n",
    "    min_time = min(times)\n",
    "    total_followers = sum(tweet['author']['followers'] for tweet in tweets)\n",
    "    total_retweets = sum(tweet['metrics']['citations']['total'] for tweet in tweets)\n",
    "    \n",
    "    avg_tweets_per_hour = num_tweets * 3600 / (max_time - min_time)\n",
    "    avg_followers_per_tweet = total_followers / num_tweets\n",
    "    avg_retweets_per_tweet = total_retweets / num_tweets\n",
    "    \n",
    "    print(filename)\n",
    "    print('Average number of tweets per hour:', avg_tweets_per_hour)\n",
    "    print('Average number of followers of users posting the tweets per tweet:', avg_followers_per_tweet)\n",
    "    print('Average number of retweets per tweet:', avg_retweets_per_tweet)\n",
    "    print('-' * 50)\n",
    "\n",
    "files = ['ECE219_tweet_data/tweets_#gohawks.txt', 'ECE219_tweet_data/tweets_#gopatriots.txt', \n",
    "         'ECE219_tweet_data/tweets_#nfl.txt', 'ECE219_tweet_data/tweets_#patriots.txt', \n",
    "         'ECE219_tweet_data/tweets_#sb49.txt', 'ECE219_tweet_data/tweets_#superbowl.txt']\n",
    "\n",
    "for file in files:\n",
    "    calculate_statistics(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea11f550-1b93-461d-80f8-c6fac79d4fa2",
   "metadata": {},
   "source": [
    "### Question 9.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5098092-f50f-4a1e-9e63-9986c17beced",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "pst_tz = pytz.timezone('America/Los_Angeles')\n",
    "\n",
    "def report_tweets(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        min_time = math.inf\n",
    "        max_time = 0\n",
    "        n_tweets = {}\n",
    "        \n",
    "        # Determine min and max time\n",
    "        for line in lines:\n",
    "            json_obj = json.loads(line)\n",
    "            citation_date = json_obj['citation_date']\n",
    "            min_time = min(min_time, citation_date)\n",
    "            max_time = max(max_time, citation_date)\n",
    "        \n",
    "        # Calculate total hours and initialize tweet count list\n",
    "        total_hours = math.ceil((max_time - min_time) / 3600)\n",
    "        n_tweets = [0] * total_hours\n",
    "        \n",
    "        # Count tweets in each hour\n",
    "        for line in lines:\n",
    "            json_obj = json.loads(line)\n",
    "            index = math.floor((json_obj['citation_date'] - min_time) / 3600)\n",
    "            n_tweets[index] += 1\n",
    "        \n",
    "        return n_tweets\n",
    "\n",
    "q2_files = ['ECE219_tweet_data/tweets_#nfl.txt','ECE219_tweet_data/tweets_#superbowl.txt']\n",
    "\n",
    "for file in q2_files:\n",
    "    n_tweets = report_tweets(file)\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.bar(range(len(n_tweets)),n_tweets)\n",
    "    plt.xlabel('Hours over time')\n",
    "    plt.ylabel('Number of tweets')\n",
    "    plt.title('Number of tweets per hour for '+file)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc24327-c7b0-498a-a9b5-949f932a8659",
   "metadata": {},
   "source": [
    "### Question 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685854cd-7e0d-4b85-8246-9149bc330bbd",
   "metadata": {},
   "source": [
    "#### Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c34b0b1-d648-4f12-827d-a0b2759acf1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   firstpost_date                                              title  \\\n",
      "0      1421210358  I've been doing this page for almost a year no...   \n",
      "1      1421244982  Good Morning #12s 4 days left, lets get it #Go...   \n",
      "2      1421250919  ‚òï üèà There's a #Storm coming #12s ‚õÖ ‚òÅ ‚ö° üèà #GBvs...   \n",
      "3      1421255373  Congrats to @DakotaDrenth on his 100th career ...   \n",
      "4      1421259406                                 #GoHawks luh Bitxh   \n",
      "\n",
      "                                                 url  \\\n",
      "0  http://twitter.com/Girlsof12/status/5552226009...   \n",
      "1  http://twitter.com/JustQ17/status/555367826077...   \n",
      "2  http://twitter.com/kingkaps7/status/5553927262...   \n",
      "3  http://twitter.com/HMSwrestling/status/5554114...   \n",
      "4  http://twitter.com/_B3NJI/status/5554283220928...   \n",
      "\n",
      "                                               tweet  \\\n",
      "0  {'contributors': None, 'truncated': False, 'te...   \n",
      "1  {'contributors': None, 'truncated': False, 'te...   \n",
      "2  {'contributors': None, 'truncated': False, 'te...   \n",
      "3  {'contributors': None, 'truncated': False, 'te...   \n",
      "4  {'contributors': None, 'truncated': False, 'te...   \n",
      "\n",
      "                                              author  \\\n",
      "0  {'author_img': 'http://pbs.twimg.com/profile_i...   \n",
      "1  {'author_img': 'http://pbs.twimg.com/profile_i...   \n",
      "2  {'author_img': 'http://pbs.twimg.com/profile_i...   \n",
      "3  {'author_img': 'http://pbs.twimg.com/profile_i...   \n",
      "4  {'author_img': 'http://pbs.twimg.com/profile_i...   \n",
      "\n",
      "                                     original_author  citation_date  \\\n",
      "0  {'author_img': 'http://pbs.twimg.com/profile_i...     1421245569   \n",
      "1  {'author_img': 'http://pbs.twimg.com/profile_i...     1421244982   \n",
      "2  {'author_img': 'http://pbs.twimg.com/profile_i...     1421250919   \n",
      "3  {'author_img': 'http://pbs.twimg.com/profile_i...     1421255373   \n",
      "4  {'author_img': 'http://pbs.twimg.com/profile_i...     1421259406   \n",
      "\n",
      "                                             metrics  \\\n",
      "0  {'acceleration': 0, 'ranking_score': 3.4600766...   \n",
      "1  {'acceleration': 0, 'ranking_score': 7.4755654...   \n",
      "2  {'acceleration': 0, 'ranking_score': 7.402587,...   \n",
      "3  {'acceleration': 0, 'ranking_score': 4.2897086...   \n",
      "4  {'acceleration': 0, 'ranking_score': 4.3229637...   \n",
      "\n",
      "                                           highlight            type  \\\n",
      "0  I've been doing this page for almost a year no...  retweet:native   \n",
      "1  Good Morning #12s 4 days left, lets get it #Go...           tweet   \n",
      "2  ‚òï üèà There's a #Storm coming #12s ‚õÖ ‚òÅ ‚ö° üèà #GBvs...           tweet   \n",
      "3  Congrats to @DakotaDrenth on his 100th career ...           tweet   \n",
      "4                                 #GoHawks luh Bitxh           tweet   \n",
      "\n",
      "                                        citation_url     label  \n",
      "0  http://twitter.com/Jamie12thlady/status/555370...  #gohawks  \n",
      "1  http://twitter.com/JustQ17/status/555367826077...  #gohawks  \n",
      "2  http://twitter.com/kingkaps7/status/5553927262...  #gohawks  \n",
      "3  http://twitter.com/HMSwrestling/status/5554114...  #gohawks  \n",
      "4  http://twitter.com/_B3NJI/status/5554283220928...  #gohawks  \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Define the data files\n",
    "data_files = ['ECE219_tweet_data/tweets_#gohawks.txt', 'ECE219_tweet_data/tweets_#gopatriots.txt', \n",
    "              'ECE219_tweet_data/tweets_#nfl.txt', 'ECE219_tweet_data/tweets_#patriots.txt', \n",
    "              'ECE219_tweet_data/tweets_#sb49.txt', 'ECE219_tweet_data/tweets_#superbowl.txt']\n",
    "\n",
    "# Define the subsampling ratio (e.g., 10%)\n",
    "subsample_ratio = 0.01\n",
    "\n",
    "# Initialize an empty list to store subsampled tweets\n",
    "subsampled_tweets = []\n",
    "\n",
    "# Iterate over each data file\n",
    "for file in data_files:\n",
    "    # Extract the label from the file name\n",
    "    label = os.path.splitext(os.path.basename(file))[0].split('_')[-1]\n",
    "    # Open the file and read each line\n",
    "    with open(file, 'r') as f:\n",
    "        # Read a subsample of tweets from each file\n",
    "        for line in f:\n",
    "            # Randomly decide whether to include the tweet in the subsample\n",
    "            if random.random() < subsample_ratio:\n",
    "                # Parse the tweet from JSON\n",
    "                tweet = json.loads(line)\n",
    "                # Add the label to the tweet\n",
    "                tweet['label'] = label\n",
    "                # Append the tweet to the subsampled list\n",
    "                subsampled_tweets.append(tweet)\n",
    "\n",
    "# Create a DataFrame from the subsampled tweets\n",
    "tweet_df = pd.DataFrame(subsampled_tweets)\n",
    "\n",
    "# Print the first few rows of the DataFrame\n",
    "print(tweet_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c8aa6f2-08ca-4c6b-b772-a7c6c9b68cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   firstpost_date                                              title  \\\n",
      "0      1421210358  I've been doing this page for almost a year no...   \n",
      "1      1421244982  Good Morning #12s 4 days left, lets get it #Go...   \n",
      "2      1421250919  ‚òï üèà There's a #Storm coming #12s ‚õÖ ‚òÅ ‚ö° üèà #GBvs...   \n",
      "3      1421255373  Congrats to @DakotaDrenth on his 100th career ...   \n",
      "4      1421259406                                 #GoHawks luh Bitxh   \n",
      "\n",
      "                                                 url  \\\n",
      "0  http://twitter.com/Girlsof12/status/5552226009...   \n",
      "1  http://twitter.com/JustQ17/status/555367826077...   \n",
      "2  http://twitter.com/kingkaps7/status/5553927262...   \n",
      "3  http://twitter.com/HMSwrestling/status/5554114...   \n",
      "4  http://twitter.com/_B3NJI/status/5554283220928...   \n",
      "\n",
      "                                               tweet  \\\n",
      "0  {'contributors': None, 'truncated': False, 'te...   \n",
      "1  {'contributors': None, 'truncated': False, 'te...   \n",
      "2  {'contributors': None, 'truncated': False, 'te...   \n",
      "3  {'contributors': None, 'truncated': False, 'te...   \n",
      "4  {'contributors': None, 'truncated': False, 'te...   \n",
      "\n",
      "                                              author  \\\n",
      "0  {'author_img': 'http://pbs.twimg.com/profile_i...   \n",
      "1  {'author_img': 'http://pbs.twimg.com/profile_i...   \n",
      "2  {'author_img': 'http://pbs.twimg.com/profile_i...   \n",
      "3  {'author_img': 'http://pbs.twimg.com/profile_i...   \n",
      "4  {'author_img': 'http://pbs.twimg.com/profile_i...   \n",
      "\n",
      "                                     original_author  citation_date  \\\n",
      "0  {'author_img': 'http://pbs.twimg.com/profile_i...     1421245569   \n",
      "1  {'author_img': 'http://pbs.twimg.com/profile_i...     1421244982   \n",
      "2  {'author_img': 'http://pbs.twimg.com/profile_i...     1421250919   \n",
      "3  {'author_img': 'http://pbs.twimg.com/profile_i...     1421255373   \n",
      "4  {'author_img': 'http://pbs.twimg.com/profile_i...     1421259406   \n",
      "\n",
      "                                             metrics  \\\n",
      "0  {'acceleration': 0, 'ranking_score': 3.4600766...   \n",
      "1  {'acceleration': 0, 'ranking_score': 7.4755654...   \n",
      "2  {'acceleration': 0, 'ranking_score': 7.402587,...   \n",
      "3  {'acceleration': 0, 'ranking_score': 4.2897086...   \n",
      "4  {'acceleration': 0, 'ranking_score': 4.3229637...   \n",
      "\n",
      "                                           highlight            type  \\\n",
      "0  I've been doing this page for almost a year no...  retweet:native   \n",
      "1  Good Morning #12s 4 days left, lets get it #Go...           tweet   \n",
      "2  ‚òï üèà There's a #Storm coming #12s ‚õÖ ‚òÅ ‚ö° üèà #GBvs...           tweet   \n",
      "3  Congrats to @DakotaDrenth on his 100th career ...           tweet   \n",
      "4                                 #GoHawks luh Bitxh           tweet   \n",
      "\n",
      "                                        citation_url     label  \\\n",
      "0  http://twitter.com/Jamie12thlady/status/555370...  #gohawks   \n",
      "1  http://twitter.com/JustQ17/status/555367826077...  #gohawks   \n",
      "2  http://twitter.com/kingkaps7/status/5553927262...  #gohawks   \n",
      "3  http://twitter.com/HMSwrestling/status/5554114...  #gohawks   \n",
      "4  http://twitter.com/_B3NJI/status/5554283220928...  #gohawks   \n",
      "\n",
      "         firstpost_date_pst         citation_date_pst  \n",
      "0 2015-01-13 20:39:18-08:00 2015-01-14 06:26:09-08:00  \n",
      "1 2015-01-14 06:16:22-08:00 2015-01-14 06:16:22-08:00  \n",
      "2 2015-01-14 07:55:19-08:00 2015-01-14 07:55:19-08:00  \n",
      "3 2015-01-14 09:09:33-08:00 2015-01-14 09:09:33-08:00  \n",
      "4 2015-01-14 10:16:46-08:00 2015-01-14 10:16:46-08:00  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pytz\n",
    "\n",
    "# Assuming 'tweet_df' is your DataFrame containing the tweet data\n",
    "# Let's say you have two columns 'firstpost_date' and 'citation_date' containing UNIX timestamps\n",
    "\n",
    "# Define the PST time zone\n",
    "pst_tz = pytz.timezone('America/Los_Angeles')\n",
    "\n",
    "# Convert UNIX timestamps to datetime objects in PST time zone\n",
    "tweet_df['firstpost_date_pst'] = pd.to_datetime(tweet_df['firstpost_date'], unit='s', utc=True).dt.tz_convert(pst_tz)\n",
    "tweet_df['citation_date_pst'] = pd.to_datetime(tweet_df['citation_date'], unit='s', utc=True).dt.tz_convert(pst_tz)\n",
    "\n",
    "# Print the DataFrame to verify the changes\n",
    "print(tweet_df.head())\n",
    "\n",
    "# Save the DataFrame as a CSV file\n",
    "tweet_df.to_csv('tweet_data_with_pst.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6509faf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   firstpost_date  \\\n",
      "0      1421210358   \n",
      "\n",
      "                                                                                                                                        title  \\\n",
      "0  I've been doing this page for almost a year now and there are 2 ladies that get the most favorites. @jaismiles and @Jamie12thlady #GoHawks   \n",
      "\n",
      "                                                      url  \\\n",
      "0  http://twitter.com/Girlsof12/status/555222600930975745   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             tweet  \\\n",
      "0  {'contributors': None, 'truncated': False, 'text': 'I've been doing this page for almost a year now and there are 2 ladies that get the most favorites. @jaismiles and @Jamie12thlady #GoHawks', 'in_reply_to_status_id': None, 'id': 555222600930975745, 'favorite_count': 0, 'source': '<a href=\"http://twitter.com/download/iphone\" rel=\"nofollow\">Twitter for iPhone</a>', 'retweeted': False, 'coordinates': None, 'timestamp_ms': '1421210358256', 'entities': {'symbols': [], 'user_mentions': [{'indices': [100, 110], 'id_str': '30713790', 'screen_name': 'jaismiles', 'name': 'Jaismiles', 'id': 30713790}, {'indices': [115, 129], 'id_str': '2306951648', 'screen_name': 'Jamie12thlady', 'name': 'Jamie 12th Lady', 'id': 2306951648}], 'trends': [], 'hashtags': [{'indices': [130, 138], 'text': 'GoHawks'}], 'urls': []}, 'in_reply_to_screen_name': None, 'in_reply_to_user_id': None, 'retweet_count': 0, 'id_str': '555222600930975745', 'favorited': False, 'user': {'follow_request_sent': None, 'profile_use_background_image': True, 'geo_enabled': False, 'description': 'Dedicated to the loud, the proud, the 12th women of the WORLD CHAMPION Seattle Seahawks! #G12', 'verified': False, 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/536610980520656897/bB3TR528_normal.jpeg', 'profile_sidebar_fill_color': 'DDEEF6', 'is_translator': False, 'id': 2445183199, 'profile_text_color': '333333', 'followers_count': 875, 'profile_sidebar_border_color': 'C0DEED', 'id_str': '2445183199', 'default_profile_image': False, 'location': 'www.girlsofthe12@gmail.com', 'utc_offset': None, 'statuses_count': 2430, 'profile_background_color': 'C0DEED', 'friends_count': 634, 'profile_link_color': '0084B4', 'profile_image_url': 'http://pbs.twimg.com/profile_images/536610980520656897/bB3TR528_normal.jpeg', 'notifications': None, 'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme1/bg.png', 'profile_banner_url': 'https://pbs.twimg.com/profile_banners/2445183199/1419831752', 'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme1/bg.png', 'name': 'Girls of 12', 'lang': 'en', 'profile_background_tile': False, 'favourites_count': 813, 'screen_name': 'Girlsof12', 'url': None, 'created_at': 'Tue Apr 15 09:25:56 +0000 2014', 'contributors_enabled': False, 'time_zone': None, 'protected': False, 'default_profile': True, 'following': None, 'listed_count': 22}, 'geo': None, 'in_reply_to_user_id_str': None, 'possibly_sensitive': False, 'lang': 'en', 'created_at': 'Wed Jan 14 04:39:18 +0000 2015', 'filter_level': 'low', 'in_reply_to_status_id_str': None, 'place': None}   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               author  \\\n",
      "0  {'author_img': 'http://pbs.twimg.com/profile_images/564565563284791296/iP_jPNd6_normal.jpeg', 'name': 'Jamie 12th Lady', 'url': 'http://twitter.com/jamie12thlady', 'nick': 'jamie12thlady', 'followers': 2601.0, 'image_url': 'http://pbs.twimg.com/profile_images/564565563284791296/iP_jPNd6_normal.jpeg', 'type': 'twitter', 'description': '#Seahawks luv from day 1 Pisces so I may be feisty but I still love you up for a good debate but I dont hate Funny FUN and my Hat is by @FanHatics CHECK EM OUT'}   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                         original_author  \\\n",
      "0  {'author_img': 'http://pbs.twimg.com/profile_images/562712260410634241/zgyJJlzm_normal.jpeg', 'description': 'Dedicated to the loud, the proud, the 12th women of the Seattle Seahawks! Super Bowl XLVIII Champions. AVI is @RileyyValentine', 'url': 'http://twitter.com/girlsof12', 'nick': 'girlsof12', 'followers': 1071.0, 'image_url': 'http://pbs.twimg.com/profile_images/562712260410634241/zgyJJlzm_normal.jpeg', 'type': 'twitter', 'name': 'Girls of 12'}   \n",
      "\n",
      "   citation_date  \\\n",
      "0     1421245569   \n",
      "\n",
      "                                                                                                                                                                                                                                  metrics  \\\n",
      "0  {'acceleration': 0, 'ranking_score': 3.4600766, 'citations': {'influential': 0, 'total': 4, 'data': [{'timestamp': 1421245559, 'citations': 1}], 'matching': 1, 'replies': 0}, 'peak': 1421245619, 'impressions': 2440, 'momentum': 1}   \n",
      "\n",
      "                                                                                                                                    highlight  \\\n",
      "0  I've been doing this page for almost a year now and there are 2 ladies that get the most favorites. @jaismiles and @Jamie12thlady #GoHawks   \n",
      "\n",
      "             type                                                citation_url  \\\n",
      "0  retweet:native  http://twitter.com/Jamie12thlady/status/555370288922107904   \n",
      "\n",
      "      label        firstpost_date_pst         citation_date_pst  \n",
      "0  #gohawks 2015-01-13 20:39:18-08:00 2015-01-14 06:26:09-08:00  \n"
     ]
    }
   ],
   "source": [
    "# Set the max_colwidth parameter to None to display entire contents of the metrics column\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Print the first row of the DataFrame\n",
    "print(tweet_df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d40d4f0",
   "metadata": {},
   "source": [
    "#### Clean Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8f0e74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Time of the day Number of tweets Number of retweets Number of followers  \\\n",
      "0                 12                1                  5              1752.0   \n",
      "1                 12                1                  2               258.0   \n",
      "2                 22                1                  5                22.0   \n",
      "3                 22                1                  2                22.0   \n",
      "4                 22                1                  2                22.0   \n",
      "...              ...              ...                ...                 ...   \n",
      "8452              20                1                  1              1802.0   \n",
      "8453              20                1                  1               200.0   \n",
      "8454              20                1                  1                11.0   \n",
      "8455              20                1                  1                99.0   \n",
      "8456              20                1                  1                27.0   \n",
      "\n",
      "                                                                                                                                     Tweet text  \\\n",
      "0                                                                                          I &lt;3 our defense! #GoHawks http://t.co/U1pcXpEsR8   \n",
      "1                                                                             twelfth dogs are ready! #gohawks #dogslife http://t.co/gd3v6vQps5   \n",
      "2                             \"Oh no big deal, just NFC West Champs and the number 1 seed again\" #Seahawks #GoHawks #12s http://t.co/7NigOjTPz2   \n",
      "3     At http://t.co/Vd0RWOeAed -- #Seahawks #12thMAN #12 #SeahawkNation #SuperBowlBound #Superbowl #Repeat #GoHawks ... http://t.co/XSEFUKqEhN   \n",
      "4                                                                          Good luck at Michigan, Jim Harbaugh. #GoHawks http://t.co/ghpshHRnON   \n",
      "...                                                                                                                                         ...   \n",
      "8452                                                  So glad it is Friday. And even better because it's a big #BlueFriday! #TGIBF #GoHawks üèàüíôüíö   \n",
      "8453                                #TGIBF #GoHawks #12thMan #Louder #WhatsNext #WhyNotUs #ImIn #RePete #Seahawks üíöüíôüíöüíôüíöüíô http://t.co/3zTvJJDbDK   \n",
      "8454                                                                  Real fans don't avoid cheese, they DEVOUR it! #Seahawks #GoHawks #GBvsSEA   \n",
      "8455                           @BrettGreene Family fun Beat the Pack song! #nopackno #gohawks Share for Hawks? Thanks!  https://t.co/oqGl8tzh1S   \n",
      "8456                                                                           Ta dificil de segurar a ansiedade pra final da NFC!!!!\\n#GoHawks   \n",
      "\n",
      "                                                                               Hashtag  \n",
      "0                                                                              gohawks  \n",
      "1                                                                    gohawks, dogslife  \n",
      "2                                                               gohawks, 12s, seahawks  \n",
      "3     12, seahawks, superbowlbound, repeat, gohawks, seahawknation, superbowl, 12thman  \n",
      "4                                                                              gohawks  \n",
      "...                                                                                ...  \n",
      "8452                                           gohawks, bluefriday, tgibf, bluefriday!  \n",
      "8453      whatsnext, seahawks, whynotus, imin, repete, louder, gohawks, 12thman, tgibf  \n",
      "8454                                                        gohawks, gbvssea, seahawks  \n",
      "8455                                                                 gohawks, nopackno  \n",
      "8456                                                                           gohawks  \n",
      "\n",
      "[8457 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import datetime\n",
    "import math\n",
    "import os\n",
    "\n",
    "# Define the data files\n",
    "data_file = 'ECE219_tweet_data/tweets_#gohawks.txt'  # Choose one data file for testing\n",
    "\n",
    "# Initialize an empty DataFrame to store features\n",
    "feature_df = pd.DataFrame(columns=['Time of the day', 'Number of tweets', \n",
    "                                   'Number of retweets', 'Number of followers', \n",
    "                                   'Tweet text', 'Hashtag'])\n",
    "\n",
    "\n",
    "# Function to extract features and add to DataFrame\n",
    "def report_features_to_df(filename, percent=5):\n",
    "    with open(filename, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        \n",
    "        # Calculate the number of lines to process based on the percentage\n",
    "        num_lines_to_process = math.ceil(len(lines) * percent / 100)\n",
    "        \n",
    "        # Initialize an empty DataFrame to store features\n",
    "        features = pd.DataFrame(columns=['Time of the day', 'Number of tweets', \n",
    "                                         'Number of retweets', 'Number of followers', \n",
    "                                         'Tweet text', 'Hashtag'])\n",
    "        \n",
    "        # Process each line up to the calculated number of lines to process\n",
    "        for i in range(num_lines_to_process):\n",
    "            json_obj = json.loads(lines[i])\n",
    "            \n",
    "            # Add features to DataFrame\n",
    "            index = i  # Assuming the index of the first row is 0\n",
    "            features.loc[index, 'Time of the day'] = datetime.datetime.fromtimestamp(json_obj['citation_date']).hour\n",
    "            features.loc[index, 'Number of tweets'] = 1\n",
    "            features.loc[index, 'Number of retweets'] = json_obj['metrics']['citations']['total']\n",
    "            features.loc[index, 'Number of followers'] = json_obj['author']['followers']\n",
    "            features.loc[index, 'Tweet text'] = json_obj['tweet']['text']\n",
    "            # Extract hashtags from entities, hashtags, and tweet text\n",
    "            hashtags = set()\n",
    "            if 'entities' in json_obj['tweet'] and 'hashtags' in json_obj['tweet']['entities']:\n",
    "                for hashtag in json_obj['tweet']['entities']['hashtags']:\n",
    "                    hashtags.add(hashtag['text'].lower())\n",
    "            if 'hashtags' in json_obj['tweet'] and json_obj['tweet']['hashtags'] is not None:\n",
    "                for hashtag in json_obj['tweet']['hashtags']:\n",
    "                    hashtags.add(hashtag.lower())\n",
    "            if 'text' in json_obj['tweet']:\n",
    "                text = json_obj['tweet']['text']\n",
    "                for word in text.split():\n",
    "                    if word.startswith('#'):\n",
    "                        hashtags.add(word[1:].lower())\n",
    "            features.loc[index, 'Hashtag'] = ', '.join(hashtags) if hashtags else None\n",
    "        \n",
    "        return features\n",
    "\n",
    "# Extract features from 5% of the data file\n",
    "features_df = report_features_to_df(data_file, percent=5)\n",
    "\n",
    "# Print the extracted features\n",
    "print(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05c65b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import datetime\n",
    "import math\n",
    "import os\n",
    "\n",
    "# Initialize an empty DataFrame to store features\n",
    "feature_df = pd.DataFrame(columns=['Time of the day', 'Number of tweets', \n",
    "                                   'Number of retweets', 'Number of followers', \n",
    "                                   'Tweet text', 'Hashtag'])\n",
    "\n",
    "# Function to extract features and add to DataFrame\n",
    "def report_features_to_df(filename, percent=5):\n",
    "    # Extract hashtag label from the file name\n",
    "    hashtag_label = os.path.basename(filename).split('_')[-1].split('.')[0][1:]\n",
    "    \n",
    "    with open(filename, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        \n",
    "        # Calculate the number of lines to process based on the percentage\n",
    "        num_lines_to_process = math.ceil(len(lines) * percent / 100)\n",
    "        \n",
    "        # Initialize an empty DataFrame to store features\n",
    "        features = pd.DataFrame(columns=['Time of the day', 'Number of tweets', \n",
    "                                         'Number of retweets', 'Number of followers', \n",
    "                                         'Tweet text', 'Hashtag', 'Hashtag Label'])\n",
    "        \n",
    "        # Process each line up to the calculated number of lines to process\n",
    "        for i in range(num_lines_to_process):\n",
    "            json_obj = json.loads(lines[i])\n",
    "            \n",
    "            # Add features to DataFrame\n",
    "            index = i  # Assuming the index of the first row is 0\n",
    "            features.loc[index, 'Time of the day'] = datetime.datetime.fromtimestamp(json_obj['citation_date']).hour\n",
    "            features.loc[index, 'Number of tweets'] = 1\n",
    "            features.loc[index, 'Number of retweets'] = json_obj['metrics']['citations']['total']\n",
    "            features.loc[index, 'Number of followers'] = json_obj['author']['followers']\n",
    "            features.loc[index, 'Tweet text'] = json_obj['tweet']['text']\n",
    "            # Extract hashtags from entities, hashtags, and tweet text\n",
    "            hashtags = set()\n",
    "            if 'entities' in json_obj['tweet'] and 'hashtags' in json_obj['tweet']['entities']:\n",
    "                for hashtag in json_obj['tweet']['entities']['hashtags']:\n",
    "                    hashtags.add(hashtag['text'].lower())\n",
    "            if 'hashtags' in json_obj['tweet'] and json_obj['tweet']['hashtags'] is not None:\n",
    "                for hashtag in json_obj['tweet']['hashtags']:\n",
    "                    hashtags.add(hashtag.lower())\n",
    "            if 'text' in json_obj['tweet']:\n",
    "                text = json_obj['tweet']['text']\n",
    "                for word in text.split():\n",
    "                    if word.startswith('#'):\n",
    "                        hashtags.add(word[1:].lower())\n",
    "            features.loc[index, 'Hashtag'] = ', '.join(hashtags) if hashtags else None\n",
    "            features.loc[index, 'Hashtag Label'] = hashtag_label\n",
    "        \n",
    "        return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5404421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Time of the day Number of tweets Number of retweets  \\\n",
      "0                   12                1                  5   \n",
      "1                   12                1                  2   \n",
      "2                   22                1                  5   \n",
      "3                   22                1                  2   \n",
      "4                   22                1                  2   \n",
      "...                ...              ...                ...   \n",
      "141186               8                1                  3   \n",
      "141187               8                1                  1   \n",
      "141188               8                1                  1   \n",
      "141189               8                1                  1   \n",
      "141190               8                1                  1   \n",
      "\n",
      "       Number of followers                                         Tweet text  \\\n",
      "0                   1752.0  I &lt;3 our defense! #GoHawks http://t.co/U1pc...   \n",
      "1                    258.0  twelfth dogs are ready! #gohawks #dogslife htt...   \n",
      "2                     22.0  \"Oh no big deal, just NFC West Champs and the ...   \n",
      "3                     22.0  At http://t.co/Vd0RWOeAed -- #Seahawks #12thMA...   \n",
      "4                     22.0  Good luck at Michigan, Jim Harbaugh. #GoHawks ...   \n",
      "...                    ...                                                ...   \n",
      "141186               141.0  @stevedarling I am with you Tom says he didn't...   \n",
      "141187            157645.0  Tickets on sale This Tuesday @ 12est.It's offi...   \n",
      "141188               129.0  #MarshawnLynch is JUST 'BOUT DAT ACTION BOSS! ...   \n",
      "141189               238.0  If there ever was a time for a Ball Inflator c...   \n",
      "141190              1696.0  El #entrenador de los @Patriots #niega saber d...   \n",
      "\n",
      "                                                  Hashtag Hashtag Label  \n",
      "0                                                 gohawks       gohawks  \n",
      "1                                       gohawks, dogslife       gohawks  \n",
      "2                                  12s, gohawks, seahawks       gohawks  \n",
      "3       superbowlbound, repeat, gohawks, seahawks, sup...       gohawks  \n",
      "4                                                 gohawks       gohawks  \n",
      "...                                                   ...           ...  \n",
      "141186                         superbowlxlix, deflategate     superbowl  \n",
      "141187                                      superbowlxlix     superbowl  \n",
      "141188  seahawks, superbowlxlix, superbowl, marshawnly...     superbowl  \n",
      "141189   superbowl....it, superbowl, superbowlcommercials     superbowl  \n",
      "141190  entrenador, internacional, superbowlxlix, nieg...     superbowl  \n",
      "\n",
      "[141191 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define the data files\n",
    "data_files = ['ECE219_tweet_data/tweets_#gohawks.txt', 'ECE219_tweet_data/tweets_#gopatriots.txt', \n",
    "              'ECE219_tweet_data/tweets_#nfl.txt', 'ECE219_tweet_data/tweets_#patriots.txt', \n",
    "              'ECE219_tweet_data/tweets_#sb49.txt', 'ECE219_tweet_data/tweets_#superbowl.txt']\n",
    "\n",
    "# Initialize an empty DataFrame to store combined features\n",
    "combined_features_df = pd.DataFrame(columns=['Time of the day', 'Number of tweets', \n",
    "                                             'Number of retweets', 'Number of followers', \n",
    "                                             'Tweet text', 'Hashtag', 'Hashtag Label'])\n",
    "\n",
    "# Extract features from each data file and concatenate them\n",
    "for file in data_files:\n",
    "    features_df = report_features_to_df(file, percent=5)\n",
    "    combined_features_df = pd.concat([combined_features_df, features_df])\n",
    "\n",
    "# Reset index of the combined dataframe\n",
    "combined_features_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Print the combined features dataframe\n",
    "print(combined_features_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1c47ba-db52-42b7-bc74-aa77c4f58462",
   "metadata": {},
   "source": [
    "#### Hashtag Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "18b10d6b-54d3-4065-b62c-8e16853ea89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     gohawks       0.90      0.93      0.91      1681\n",
      "  gopatriots       0.87      0.84      0.86       223\n",
      "         nfl       0.82      0.78      0.80      2331\n",
      "    patriots       0.89      0.88      0.88      4470\n",
      "        sb49       0.99      0.99      0.99      7437\n",
      "   superbowl       0.95      0.97      0.96     12097\n",
      "\n",
      "    accuracy                           0.94     28239\n",
      "   macro avg       0.91      0.90      0.90     28239\n",
      "weighted avg       0.94      0.94      0.94     28239\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming combined_features_df is already loaded as a DataFrame\n",
    "data = combined_features_df\n",
    "\n",
    "# Encode 'Tweet text' using TF-IDF\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "X_text = tfidf.fit_transform(data['Tweet text'])\n",
    "\n",
    "# Convert numerical features to appropriate data types\n",
    "numerical_features = ['Number of tweets', 'Number of retweets', 'Number of followers', 'Time of the day']\n",
    "for feature in numerical_features:\n",
    "    data[feature] = pd.to_numeric(data[feature], errors='coerce')  # coerce invalid parsing to NaN\n",
    "\n",
    "# Remove rows with NaN values\n",
    "data.dropna(subset=numerical_features, inplace=True)\n",
    "\n",
    "# Convert numerical features to sparse format\n",
    "X_num_sparse = scipy.sparse.csr_matrix(data[numerical_features].values)\n",
    "\n",
    "# Concatenate numerical and textual features\n",
    "X = scipy.sparse.hstack([X_text, X_num_sparse])\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(data['Hashtag Label'])\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a RandomForest model\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74e14ff-fb0b-446c-958a-0e82624258b0",
   "metadata": {},
   "source": [
    "#### Hashtag Classification Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "53003689-1bbc-4ca1-b7c4-d5e1551593ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffled Training Labels:\n",
      "[3 5 2 ... 4 2 5]\n",
      "\n",
      "Class Distribution in the Training Data:\n",
      "{0: 0.05999008428358949, 1: 0.008437212267157731, 2: 0.08252177916283022, 3: 0.1554819746440966, 4: 0.2633508038812947, 5: 0.43021814576103123}\n",
      "\n",
      "Baseline Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     gohawks       0.06      0.06      0.06      1681\n",
      "  gopatriots       0.01      0.01      0.01       223\n",
      "         nfl       0.10      0.10      0.10      2331\n",
      "    patriots       0.15      0.15      0.15      4470\n",
      "        sb49       0.26      0.26      0.26      7437\n",
      "   superbowl       0.42      0.43      0.43     12097\n",
      "\n",
      "    accuracy                           0.29     28239\n",
      "   macro avg       0.17      0.17      0.17     28239\n",
      "weighted avg       0.29      0.29      0.29     28239\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Shuffle the training labels\n",
    "shuffled_y_train = shuffle(y_train, random_state=42)\n",
    "\n",
    "# Print the shuffled labels\n",
    "print(\"Shuffled Training Labels:\")\n",
    "print(shuffled_y_train)\n",
    "\n",
    "# Calculate the class distribution in the training data\n",
    "class_distribution = {label: count / len(shuffled_y_train) for label, count in zip(*np.unique(shuffled_y_train, return_counts=True))}\n",
    "print(\"\\nClass Distribution in the Training Data:\")\n",
    "print(class_distribution)\n",
    "\n",
    "# Randomly assign labels based on the class distribution\n",
    "random_pred_baseline = np.random.choice(list(class_distribution.keys()), size=len(y_test), p=list(class_distribution.values()))\n",
    "\n",
    "# Print classification report for the baseline model\n",
    "print(\"\\nBaseline Classification Report:\")\n",
    "print(classification_report(y_test, random_pred_baseline, target_names=le.classes_, zero_division=1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa00153f-961b-4f1b-aa9a-ee513ada093b",
   "metadata": {},
   "source": [
    "#### Retweet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c22146c0-5752-4767-a996-002a705508a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 5.535046352890463\n",
      "Mean Squared Error: 365.61456975189583\n",
      "Root Mean Squared Error: 19.121050435368236\n",
      "R-squared (R^2) Score: -0.6435524383198834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/jupyterlab/4.0.7_1/libexec/lib/python3.11/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming combined_features_df is already loaded as a DataFrame\n",
    "data = combined_features_df\n",
    "\n",
    "# Feature Engineering\n",
    "# Encode 'Tweet text' using TF-IDF\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "X_text = tfidf.fit_transform(data['Tweet text'])\n",
    "\n",
    "# Include numerical features\n",
    "X_num = data[['Number of followers', 'Time of the day']]  # Add other relevant numerical features\n",
    "\n",
    "# Combine textual and numerical features\n",
    "X = pd.concat([pd.DataFrame(X_text.toarray()), X_num], axis=1)\n",
    "\n",
    "# Convert feature names to strings\n",
    "X.columns = X.columns.astype(str)\n",
    "\n",
    "# Target variable\n",
    "y = data['Number of retweets']  # Replace 'Number of retweets' with 'Number of likes' or 'Number of quotes' as needed\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a simpler model like Linear Regression\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Compute Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "# Compute Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Compute Root Mean Squared Error (RMSE)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "\n",
    "# Compute R-squared (R^2) score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R-squared (R^2) Score:\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea91cf8-e267-43ea-994f-107eaadf5726",
   "metadata": {},
   "source": [
    "#### Optimize the Model for Better Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8acd1ca3-3f8d-4e30-b0f9-5c18092a87b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding 'Tweet text' using TF-IDF...\n",
      "Parsing numerical features...\n",
      "Converting numerical features to sparse format...\n",
      "Combining textual and numerical features...\n",
      "Standardizing numerical features...\n",
      "Applying PCA for Dimensionality Reduction...\n",
      "Splitting the data into train and test sets...\n",
      "Training Random Forest Regressor model with hyperparameter tuning...\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "[CV] END .max_depth=10, min_samples_split=5, n_estimators=50; total time= 6.0min\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=13.0min\n",
      "[CV] END .max_depth=20, min_samples_split=5, n_estimators=50; total time=13.3min\n",
      "[CV] END .max_depth=10, min_samples_split=2, n_estimators=50; total time= 6.1min\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=13.1min\n",
      "[CV] END .max_depth=20, min_samples_split=5, n_estimators=50; total time=13.4min\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=12.6min\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=25.7min\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=12.6min\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=26.1min\n",
      "[CV] END .max_depth=10, min_samples_split=2, n_estimators=50; total time= 6.1min\n",
      "[CV] END .max_depth=20, min_samples_split=2, n_estimators=50; total time=12.4min\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=25.2min\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=12.5min\n",
      "[CV] END .max_depth=20, min_samples_split=2, n_estimators=50; total time=12.4min\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=22.9min\n",
      "[CV] END .max_depth=10, min_samples_split=2, n_estimators=50; total time= 6.0min\n",
      "[CV] END .max_depth=10, min_samples_split=5, n_estimators=50; total time= 6.5min\n",
      "[CV] END .max_depth=20, min_samples_split=2, n_estimators=50; total time=12.6min\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=23.0min\n",
      "[CV] END .max_depth=10, min_samples_split=5, n_estimators=50; total time= 6.0min\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=13.0min\n",
      "[CV] END .max_depth=20, min_samples_split=5, n_estimators=50; total time=13.1min\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=19.6min\n",
      "Making predictions on the test set...\n",
      "Computing evaluation metrics...\n",
      "Evaluation Metrics:\n",
      "Mean Absolute Error: 2.3306063208218726\n",
      "Mean Squared Error: 319.5926803656248\n",
      "Root Mean Squared Error: 17.877155264907916\n",
      "R-squared (R^2) Score: -0.43666957649021865\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "\n",
    "# Assuming combined_features_df is already loaded as a DataFrame\n",
    "data = combined_features_df\n",
    "\n",
    "# Encode 'Tweet text' using TF-IDF\n",
    "print(\"Encoding 'Tweet text' using TF-IDF...\")\n",
    "tfidf = TfidfVectorizer(max_features=500)  # Reduced number of features\n",
    "X_text = tfidf.fit_transform(data['Tweet text'])\n",
    "\n",
    "# Ensure numerical features are correctly parsed\n",
    "print(\"Parsing numerical features...\")\n",
    "numerical_features = ['Number of followers', 'Time of the day']  # Add other relevant numerical features\n",
    "\n",
    "# Convert numerical features to appropriate data types and handle missing values\n",
    "for feature in numerical_features:\n",
    "    data[feature] = pd.to_numeric(data[feature], errors='coerce')  # Convert to numeric, coerce errors to NaN\n",
    "\n",
    "# Remove rows with NaN values in numerical features\n",
    "data.dropna(subset=numerical_features, inplace=True)\n",
    "\n",
    "# Extract numerical features after ensuring they are numeric\n",
    "X_num = data[numerical_features].values\n",
    "\n",
    "# Convert numerical features to sparse format\n",
    "print(\"Converting numerical features to sparse format...\")\n",
    "X_num_sparse = scipy.sparse.csr_matrix(X_num)\n",
    "\n",
    "# Combine textual and numerical features\n",
    "print(\"Combining textual and numerical features...\")\n",
    "X_combined = scipy.sparse.hstack([X_text, X_num_sparse])\n",
    "\n",
    "# Standardize numerical features\n",
    "print(\"Standardizing numerical features...\")\n",
    "scaler = StandardScaler(with_mean=False)  # Set with_mean=False for sparse matrices\n",
    "X_combined[:, -X_num.shape[1]:] = scaler.fit_transform(X_combined[:, -X_num.shape[1]:])\n",
    "\n",
    "# Apply PCA for Dimensionality Reduction\n",
    "print(\"Applying PCA for Dimensionality Reduction...\")\n",
    "pca = PCA(n_components=100)  # Reduced number of components\n",
    "X = pca.fit_transform(X_combined.toarray())\n",
    "\n",
    "# Target variable\n",
    "y = data['Number of retweets']\n",
    "\n",
    "# Split the data into train and test sets\n",
    "print(\"Splitting the data into train and test sets...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest Regressor model with hyperparameter tuning using RandomizedSearchCV\n",
    "print(\"Training Random Forest Regressor model with hyperparameter tuning...\")\n",
    "param_distributions = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [10, 20],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "# There are 2 * 2 * 2 = 8 combinations in the search space\n",
    "n_iter = 8  # Match the number of parameter combinations\n",
    "\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_distributions, n_iter=n_iter, cv=3, n_jobs=-1, verbose=2)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "best_rf = random_search.best_estimator_\n",
    "\n",
    "# Predict on the test set\n",
    "print(\"Making predictions on the test set...\")\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "# Compute evaluation metrics\n",
    "print(\"Computing evaluation metrics...\")\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R-squared (R^2) Score:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fb7032-a0bf-4564-b299-3b5a1ca0a0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "\n",
    "# Assuming combined_features_df is already loaded as a DataFrame\n",
    "data = combined_features_df\n",
    "\n",
    "# Encode 'Tweet text' using TF-IDF\n",
    "print(\"Encoding 'Tweet text' using TF-IDF...\")\n",
    "tfidf = TfidfVectorizer(max_features=500)  # Reduced number of features\n",
    "X_text = tfidf.fit_transform(data['Tweet text'])\n",
    "\n",
    "# Ensure numerical features are correctly parsed\n",
    "print(\"Parsing numerical features...\")\n",
    "numerical_features = ['Number of followers', 'Time of the day']  # Add other relevant numerical features\n",
    "\n",
    "# Convert numerical features to appropriate data types and handle missing values\n",
    "for feature in numerical_features:\n",
    "    data[feature] = pd.to_numeric(data[feature], errors='coerce')  # Convert to numeric, coerce errors to NaN\n",
    "\n",
    "# Remove rows with NaN values in numerical features\n",
    "data.dropna(subset=numerical_features, inplace=True)\n",
    "\n",
    "# Extract numerical features after ensuring they are numeric\n",
    "X_num = data[numerical_features].values\n",
    "\n",
    "# Convert numerical features to sparse format\n",
    "print(\"Converting numerical features to sparse format...\")\n",
    "X_num_sparse = scipy.sparse.csr_matrix(X_num)\n",
    "\n",
    "# Combine textual and numerical features\n",
    "print(\"Combining textual and numerical features...\")\n",
    "X_combined = scipy.sparse.hstack([X_text, X_num_sparse])\n",
    "\n",
    "# Standardize numerical features\n",
    "print(\"Standardizing numerical features...\")\n",
    "scaler = StandardScaler(with_mean=False)  # Set with_mean=False for sparse matrices\n",
    "X_combined = scipy.sparse.hstack([X_text, scaler.fit_transform(X_num_sparse)])\n",
    "\n",
    "# Apply PCA for Dimensionality Reduction\n",
    "print(\"Applying PCA for Dimensionality Reduction...\")\n",
    "pca = PCA(n_components=100)  # Reduced number of components\n",
    "X = pca.fit_transform(X_combined.toarray())\n",
    "\n",
    "# Target variable\n",
    "y = data['Number of retweets']\n",
    "\n",
    "# Split the data into train and test sets\n",
    "print(\"Splitting the data into train and test sets...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Gradient Boosting Regressor model with hyperparameter tuning using RandomizedSearchCV\n",
    "print(\"Training Gradient Boosting Regressor model with hyperparameter tuning...\")\n",
    "param_distributions = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 5],\n",
    "    'learning_rate': [0.01, 0.1]\n",
    "}\n",
    "\n",
    "# There are 2 * 2 * 2 = 8 combinations in the search space\n",
    "n_iter = 8  # Match the number of parameter combinations\n",
    "\n",
    "gbr = GradientBoostingRegressor(random_state=42)\n",
    "random_search = RandomizedSearchCV(estimator=gbr, param_distributions=param_distributions, n_iter=n_iter, cv=3, n_jobs=-1, verbose=2)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "best_gbr = random_search.best_estimator_\n",
    "\n",
    "# Predict on the test set\n",
    "print(\"Making predictions on the test set...\")\n",
    "y_pred = best_gbr.predict(X_test)\n",
    "\n",
    "# Compute evaluation metrics\n",
    "print(\"Computing evaluation metrics...\")\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R-squared (R^2) Score:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80e20d2-4876-461d-b8d7-617c6f661061",
   "metadata": {},
   "source": [
    "#### Retweet Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cbccb86-ad91-49d2-bac4-e3e26e272fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Model Evaluation Metrics:\n",
      "Mean Absolute Error: 2.86311979720438\n",
      "Mean Squared Error: 222.95390778487854\n",
      "Root Mean Squared Error: 14.931641161803968\n",
      "R-squared (R^2) Score: -0.002247911021285187\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "# Assuming combined_features_df is already loaded as a DataFrame\n",
    "data = combined_features_df\n",
    "\n",
    "# Extracting target variable\n",
    "y = data['Number of retweets']\n",
    "\n",
    "# Predicting the mean value of the target variable as a baseline\n",
    "dummy_regressor = DummyRegressor(strategy='mean')\n",
    "dummy_regressor.fit(X_train, y_train)\n",
    "y_pred_baseline = dummy_regressor.predict(X_test)\n",
    "\n",
    "# Compute evaluation metrics for the baseline model\n",
    "mae_baseline = mean_absolute_error(y_test, y_pred_baseline)\n",
    "mse_baseline = mean_squared_error(y_test, y_pred_baseline)\n",
    "rmse_baseline = np.sqrt(mse_baseline)\n",
    "r2_baseline = r2_score(y_test, y_pred_baseline)\n",
    "\n",
    "# Print evaluation metrics for the baseline model\n",
    "print(\"Baseline Model Evaluation Metrics:\")\n",
    "print(\"Mean Absolute Error:\", mae_baseline)\n",
    "print(\"Mean Squared Error:\", mse_baseline)\n",
    "print(\"Root Mean Squared Error:\", rmse_baseline)\n",
    "print(\"R-squared (R^2) Score:\", r2_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6402fa-ee1e-49d7-86b2-ff31bdbef71d",
   "metadata": {},
   "source": [
    "#### Relative Time Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9cec1543-27d5-494d-8481-e8824dec2917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 6.293879696934464e-13\n",
      "Mean Squared Error: 7.7329435180892385e-25\n",
      "Root Mean Squared Error: 8.793715664091737e-13\n",
      "R-squared (R^2) Score: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/jupyterlab/4.0.7_1/libexec/lib/python3.11/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming combined_features_df is already loaded as a DataFrame\n",
    "data = combined_features_df\n",
    "\n",
    "# Feature Engineering\n",
    "# Example: Convert categorical variables to numerical using one-hot encoding\n",
    "data = pd.get_dummies(data, columns=[\"Hashtag Label\"])\n",
    "\n",
    "# Example: Extract features from text using TF-IDF\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "X_text = tfidf.fit_transform(data['Tweet text'])\n",
    "\n",
    "# Combine textual features with other numerical features\n",
    "X_num = data.drop(['Tweet text', 'Hashtag'], axis=1)\n",
    "X = pd.concat([pd.DataFrame(X_text.toarray()), X_num], axis=1)\n",
    "\n",
    "# Convert feature names to strings\n",
    "X.columns = X.columns.astype(str)\n",
    "\n",
    "# Target variable\n",
    "y = data['Time of the day']\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R-squared (R^2) Score:\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbf1a05d-2f89-456a-a7bc-33d42868267b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Model\n",
      "Mean Absolute Error: 6.293879696934464e-13\n",
      "Mean Squared Error: 7.7329435180892385e-25\n",
      "Root Mean Squared Error: 8.793715664091737e-13\n",
      "R-squared (R^2) Score: 1.0\n",
      "\n",
      "Baseline Model (Mean Predictor)\n",
      "Mean Absolute Error: 4.717447501682071\n",
      "Mean Squared Error: 32.07096568575375\n",
      "Root Mean Squared Error: 5.663123315428841\n",
      "R-squared (R^2) Score: -0.016396757844456156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/jupyterlab/4.0.7_1/libexec/lib/python3.11/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Cellar/jupyterlab/4.0.7_1/libexec/lib/python3.11/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming combined_features_df is already loaded as a DataFrame\n",
    "data = combined_features_df\n",
    "\n",
    "# Feature Engineering\n",
    "# Example: Convert categorical variables to numerical using one-hot encoding\n",
    "data = pd.get_dummies(data, columns=[\"Hashtag Label\"])\n",
    "\n",
    "# Example: Extract features from text using TF-IDF\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "X_text = tfidf.fit_transform(data['Tweet text'])\n",
    "\n",
    "# Combine textual features with other numerical features\n",
    "X_num = data.drop(['Tweet text', 'Hashtag'], axis=1)\n",
    "X = pd.concat([pd.DataFrame(X_text.toarray()), X_num.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Convert feature names to strings\n",
    "X.columns = X.columns.astype(str)\n",
    "\n",
    "# Target variable\n",
    "y = data['Time of the day']\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print evaluation metrics for Linear Regression\n",
    "print(\"Linear Regression Model\")\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R-squared (R^2) Score:\", r2)\n",
    "\n",
    "# Baseline model: Predict the mean value of the target variable\n",
    "y_baseline_pred = np.full_like(y_test, y_train.mean())\n",
    "\n",
    "# Evaluation for the baseline model\n",
    "baseline_mae = mean_absolute_error(y_test, y_baseline_pred)\n",
    "baseline_mse = mean_squared_error(y_test, y_baseline_pred)\n",
    "baseline_rmse = mean_squared_error(y_test, y_baseline_pred, squared=False)\n",
    "baseline_r2 = r2_score(y_test, y_baseline_pred)\n",
    "\n",
    "# Print evaluation metrics for Baseline Model\n",
    "print(\"\\nBaseline Model (Mean Predictor)\")\n",
    "print(\"Mean Absolute Error:\", baseline_mae)\n",
    "print(\"Mean Squared Error:\", baseline_mse)\n",
    "print(\"Root Mean Squared Error:\", baseline_rmse)\n",
    "print(\"R-squared (R^2) Score:\", baseline_r2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
