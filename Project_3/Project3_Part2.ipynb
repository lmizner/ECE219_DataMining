{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33baf61e-a0ad-429f-9723-5659c75949ea",
   "metadata": {},
   "source": [
    "# Project 3: Recommendation System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5de0fa9-0138-4929-b85f-1e2e05e8acd8",
   "metadata": {},
   "source": [
    "### Question 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "845a6c33-ce2a-42e9-ae31-69f601e8e650",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.metrics import ndcg_score\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset for one fold\n",
    "def load_one_file(data_path):\n",
    "    X_train, y_train, qid_train = load_svmlight_file(str(data_path + 'train.txt'), query_id=True)\n",
    "    X_test, y_test, qid_test = load_svmlight_file(str(data_path + 'test.txt'), query_id=True)\n",
    "    y_train = y_train.astype(int)\n",
    "    y_test = y_test.astype(int)\n",
    "    _, group_train = np.unique(qid_train, return_counts=True)\n",
    "    _, group_test = np.unique(qid_test, return_counts=True)\n",
    "    return X_train, y_train, qid_train, group_train, X_test, y_test, qid_test, group_test\n",
    "\n",
    "def ndcg_single_query(y_score, y_true, k):\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order[:k])\n",
    "\n",
    "    gain = 2 ** y_true - 1\n",
    "\n",
    "    discounts = np.log2(np.arange(len(y_true)) + 2)\n",
    "    return np.sum(gain / discounts)\n",
    "\n",
    "# calculate NDCG score given a trained model \n",
    "def compute_ndcg_all(model, X_test, y_test, qids_test, k=10):\n",
    "    unique_qids = np.unique(qids_test)\n",
    "    ndcg_ = list()\n",
    "    for i, qid in enumerate(unique_qids):\n",
    "        y = y_test[qids_test == qid]\n",
    "\n",
    "        if np.sum(y) == 0:\n",
    "            continue\n",
    "\n",
    "        p = model.predict(X_test[qids_test == qid])\n",
    "\n",
    "        idcg = ndcg_single_query(y, y, k=k)\n",
    "        ndcg_.append(ndcg_single_query(p, y, k=k) / idcg)\n",
    "    return np.mean(ndcg_)\n",
    "\n",
    "# get importance of features\n",
    "def get_feature_importance(model, importance_type='gain'):\n",
    "    return model.feature_importance(importance_type=importance_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e550f24-0c46-41f9-bdfd-87f0a7a34a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique queries in fold 1: 8000\n",
      "Distribution of relevance labels in training data for fold 1: [377957 232569  95082  12658   5146]\n",
      "Distribution of relevance labels in test data for fold 1: [124784  77896  32459   4450   1932]\n",
      "Total number of unique queries in fold 2: 8000\n",
      "Distribution of relevance labels in training data for fold 2: [373029 230368  95117  12814   5355]\n",
      "Distribution of relevance labels in test data for fold 2: [126450  78016  31875   4053   1594]\n",
      "Total number of unique queries in fold 3: 8000\n",
      "Distribution of relevance labels in training data for fold 3: [371725 232302  96663  12903   5518]\n",
      "Distribution of relevance labels in test data for fold 3: [126088  75962  30913   4361   1769]\n",
      "Total number of unique queries in fold 4: 8000\n",
      "Distribution of relevance labels in training data for fold 4: [372756 231727  96244  12712   5329]\n",
      "Distribution of relevance labels in test data for fold 4: [125419  78591  32294   4244   1783]\n",
      "Total number of unique queries in fold 5: 8000\n",
      "Distribution of relevance labels in training data for fold 5: [377322 231874  95247  12864   5295]\n",
      "Distribution of relevance labels in test data for fold 5: [121522  75815  31910   4209   1803]\n",
      "\n",
      "Aggregated results across all folds:\n",
      "Total number of unique queries: 40000\n",
      "Total distribution of relevance labels in training data across all folds: [1872789. 1158840.  478353.   63951.   26643.]\n",
      "Total distribution of relevance labels in test data across all folds: [624263. 386280. 159451.  21317.   8881.]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "data_dir = \"MSLR-WEB10K/\"\n",
    "\n",
    "total_unique_queries = 0\n",
    "total_relevance_label_distribution_train = np.zeros(5)\n",
    "total_relevance_label_distribution_test = np.zeros(5)\n",
    "\n",
    "for fold_num in range(1, 6):\n",
    "    fold_path = os.path.join(data_dir, f\"Fold{fold_num}/\")\n",
    "    # Load and preprocess the data for the current fold\n",
    "    X_train, y_train, qid_train, group_train, X_test, y_test, qid_test, group_test = load_one_file(fold_path)\n",
    "\n",
    "    unique_queries_train = np.unique(qid_train)\n",
    "    unique_queries_test = np.unique(qid_test)\n",
    "    total_unique_queries_fold = len(np.unique(np.concatenate((qid_train, qid_test))))\n",
    "    print(f\"Total number of unique queries in fold {fold_num}: {total_unique_queries_fold}\")\n",
    "    total_unique_queries += total_unique_queries_fold\n",
    "\n",
    "    # distribution of relevance labels for the current fold\n",
    "    relevance_label_distribution_train = np.bincount(y_train)\n",
    "    relevance_label_distribution_test = np.bincount(y_test)\n",
    "    print(f\"Distribution of relevance labels in training data for fold {fold_num}: {relevance_label_distribution_train}\")\n",
    "    print(f\"Distribution of relevance labels in test data for fold {fold_num}: {relevance_label_distribution_test}\")\n",
    "\n",
    "    # relevance label distributions across folds\n",
    "    total_relevance_label_distribution_train += relevance_label_distribution_train\n",
    "    total_relevance_label_distribution_test += relevance_label_distribution_test\n",
    "\n",
    "# Print Results\n",
    "print(\"\\nAggregated results across all folds:\")\n",
    "print(\"Total number of unique queries:\", total_unique_queries)\n",
    "print(\"Total distribution of relevance labels in training data across all folds:\", total_relevance_label_distribution_train)\n",
    "print(\"Total distribution of relevance labels in test data across all folds:\", total_relevance_label_distribution_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccad1dd-8619-45f9-b4b0-637af77b52a3",
   "metadata": {},
   "source": [
    "### Question 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d375112c-0d53-4d18-9500-42005114c94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.072168 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 25501\n",
      "[LightGBM] [Info] Number of data points in the train set: 722602, number of used features: 136\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.073466 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 25501\n",
      "[LightGBM] [Info] Number of data points in the train set: 722602, number of used features: 136\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.071583 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 25501\n",
      "[LightGBM] [Info] Number of data points in the train set: 722602, number of used features: 136\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.080596 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 25501\n",
      "[LightGBM] [Info] Number of data points in the train set: 722602, number of used features: 136\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.073194 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 25501\n",
      "[LightGBM] [Info] Number of data points in the train set: 722602, number of used features: 136\n",
      "Average nDCG@3 across all folds: 0.4696344288396136\n",
      "Average nDCG@5 across all folds: 0.4714315145908389\n",
      "Average nDCG@10 across all folds: 0.49035928048966515\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "data_dir = \"MSLR-WEB10K/\"\n",
    "\n",
    "ndcg_3_scores = []\n",
    "ndcg_5_scores = []\n",
    "ndcg_10_scores = []\n",
    "\n",
    "for fold_dir in os.listdir(data_dir):\n",
    "    if os.path.isdir(os.path.join(data_dir, fold_dir)):\n",
    "        # Load and preprocess the data for the current fold\n",
    "        fold_data_dir = os.path.join(data_dir, f\"Fold{fold_num}/\")\n",
    "        X_train, y_train, qid_train, group_train, X_test, y_test, qid_test, group_test = load_one_file(fold_data_dir)\n",
    "\n",
    "        # Train a LightGBM model using the 'lambdarank' objective\n",
    "        lgb_train = lgb.Dataset(X_train, label = y_train, group = group_train)\n",
    "        lgb_test = lgb.Dataset(X_test, label = y_test, group = group_test)\n",
    "        \n",
    "        params = {\n",
    "            'objective': 'lambdarank',\n",
    "            'metric': 'ndcg',\n",
    "            'ndcg_at': [3, 5, 10],\n",
    "        }\n",
    "\n",
    "        num_round = 100\n",
    "        model = lgb.train(params, lgb_train, num_round, valid_sets = [lgb_test])\n",
    "\n",
    "        # nDCG@3, nDCG@5, and nDCG@10\n",
    "        ndcg_3 = compute_ndcg_all(model, X_test, y_test, qid_test, k = 3)\n",
    "        ndcg_5 = compute_ndcg_all(model, X_test, y_test, qid_test, k = 5)\n",
    "        ndcg_10 = compute_ndcg_all(model, X_test, y_test, qid_test, k = 10)\n",
    "\n",
    "        ndcg_3_scores.append(ndcg_3)\n",
    "        ndcg_5_scores.append(ndcg_5)\n",
    "        ndcg_10_scores.append(ndcg_10)\n",
    "\n",
    "# Print average nDCG scores across all folds\n",
    "print(\"Average nDCG@3 across all folds:\", np.mean(ndcg_3_scores))\n",
    "print(\"Average nDCG@5 across all folds:\", np.mean(ndcg_5_scores))\n",
    "print(\"Average nDCG@10 across all folds:\", np.mean(ndcg_10_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0f0762-e05c-4bbe-98ff-570b01eddfce",
   "metadata": {},
   "source": [
    "### Question 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0632c3f-4f51-459a-b402-d98f1bd2ab59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.071146 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 25637\n",
      "[LightGBM] [Info] Number of data points in the train set: 723412, number of used features: 136\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.083791 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 25623\n",
      "[LightGBM] [Info] Number of data points in the train set: 716683, number of used features: 136\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.070493 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 25659\n",
      "[LightGBM] [Info] Number of data points in the train set: 719111, number of used features: 136\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.080745 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 25631\n",
      "[LightGBM] [Info] Number of data points in the train set: 718768, number of used features: 136\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.082727 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 25501\n",
      "[LightGBM] [Info] Number of data points in the train set: 722602, number of used features: 136\n",
      "Fold 1: Top 5 most important features\n",
      "   1. Feature 133\n",
      "   2. Feature 7\n",
      "   3. Feature 107\n",
      "   4. Feature 54\n",
      "   5. Feature 129\n",
      "Fold 2: Top 5 most important features\n",
      "   1. Feature 133\n",
      "   2. Feature 7\n",
      "   3. Feature 54\n",
      "   4. Feature 107\n",
      "   5. Feature 129\n",
      "Fold 3: Top 5 most important features\n",
      "   1. Feature 133\n",
      "   2. Feature 54\n",
      "   3. Feature 107\n",
      "   4. Feature 129\n",
      "   5. Feature 7\n",
      "Fold 4: Top 5 most important features\n",
      "   1. Feature 133\n",
      "   2. Feature 7\n",
      "   3. Feature 54\n",
      "   4. Feature 129\n",
      "   5. Feature 128\n",
      "Fold 5: Top 5 most important features\n",
      "   1. Feature 133\n",
      "   2. Feature 7\n",
      "   3. Feature 54\n",
      "   4. Feature 107\n",
      "   5. Feature 129\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"MSLR-WEB10K/\"\n",
    "\n",
    "top_features_per_fold = []\n",
    "\n",
    "# Loop through each fold directory\n",
    "for fold_num in range(1, 6):\n",
    "    fold_data_dir = os.path.join(data_dir, f\"Fold{fold_num}/\")\n",
    "    \n",
    "    # Load the dataset for the current fold\n",
    "    X_train, y_train, qid_train, group_train, X_test, y_test, qid_test, group_test = load_one_file(fold_data_dir)\n",
    "    \n",
    "    # Train the LightGBM model with 'lambdarank' objective\n",
    "    lgb_train = lgb.Dataset(X_train, label = y_train, group = group_train, free_raw_data = False)\n",
    "    lgb_test = lgb.Dataset(X_test, label = y_test, group = group_test, free_raw_data = False)\n",
    "    params = {\n",
    "        'objective': 'lambdarank'\n",
    "    }\n",
    "    model = lgb.train(params, lgb_train, valid_sets=[lgb_test], valid_names = ['test'])\n",
    "    importance_scores = get_feature_importance(model, importance_type = 'gain')\n",
    "\n",
    "    # Sort\n",
    "    sorted_indices = np.argsort(importance_scores)[::-1]\n",
    "    \n",
    "    # Top 5 most important features\n",
    "    top_features = sorted_indices[:5]\n",
    "    top_features_per_fold.append(top_features)\n",
    "\n",
    "# Print the top 5 most important features for each fold\n",
    "for fold_num, top_features in enumerate(top_features_per_fold, start = 1):\n",
    "    print(f\"Fold {fold_num}: Top 5 most important features\")\n",
    "    for i, feature_idx in enumerate(top_features, start = 1):\n",
    "        print(f\"   {i}. Feature {feature_idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6bbc05-5a94-4f80-9060-84142cce479c",
   "metadata": {},
   "source": [
    "### Question 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b1aa3973-4bdf-412f-8fc4-fab83b600b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of X_train: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Type of X_test: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Type of X_train: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Type of X_test: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Type of X_train: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Type of X_test: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Type of X_train: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Type of X_test: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Type of X_train: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Type of X_test: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Average nDCG score across all folds after removing top 20 features: 0.522838100527449\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import ndcg_score\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'ndcg',\n",
    "    'verbosity': -1\n",
    "}\n",
    "\n",
    "data_dir = \"MSLR-WEB10K/\"\n",
    "\n",
    "ndcg_scores = []\n",
    "\n",
    "for fold_num in range(1, 6):\n",
    "    fold_path = os.path.join(data_dir, f\"Fold{fold_num}/\")\n",
    "\n",
    "    # Load the data for the current fold\n",
    "    X_train, y_train, qid_train = load_svmlight_file(os.path.join(fold_path, 'train.txt'), query_id = True)\n",
    "    X_test, y_test, qid_test = load_svmlight_file(os.path.join(fold_path, 'test.txt'), query_id = True)\n",
    "    \n",
    "    feature_importance_scores = get_feature_importance(model)\n",
    "\n",
    "    # Filter out top feature indices\n",
    "    top_feature_indices = [idx for idx in top_feature_indices if idx < X_train.shape[1]]\n",
    "\n",
    "    X_train_csc = X_train.tocsc()\n",
    "    X_test_csc = X_test.tocsc()\n",
    "    \n",
    "    # Remove top features\n",
    "    X_train_filtered_csc = X_train_csc[:, [i for i in range(X_train_csc.shape[1]) if i not in top_feature_indices]]\n",
    "    X_test_filtered_csc = X_test_csc[:, [i for i in range(X_test_csc.shape[1]) if i not in top_feature_indices]]\n",
    "    \n",
    "    X_train_filtered = X_train_filtered_csc.tocsr()\n",
    "    X_test_filtered = X_test_filtered_csc.tocsr()\n",
    "\n",
    "    group_train = np.bincount(qid_train)\n",
    "    group_test = np.bincount(qid_test)\n",
    "    \n",
    "    # Create LightGBM datasets\n",
    "    lgb_train = lgb.Dataset(X_train_filtered, y_train, group = group_train)\n",
    "    lgb_test = lgb.Dataset(X_test_filtered, y_test, reference = lgb_train, group = group_test)\n",
    "\n",
    "    # Train LightGBM model\n",
    "    model = lgb.train(params, lgb_train, num_boost_round = 100)\n",
    "\n",
    "    y_pred = model.predict(X_test_filtered)\n",
    "\n",
    "    # Calculate nDCG score for this fold\n",
    "    ndcg = ndcg_score([y_test], [y_pred], k = 10) \n",
    "    ndcg_scores.append(ndcg)\n",
    "\n",
    "# Calculate average nDCG score across all folds\n",
    "average_ndcg = np.mean(ndcg_scores)\n",
    "print(\"Average nDCG score across all folds after removing top 20 features:\", average_ndcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cf47e01f-983b-4f50-9c36-59c978cf7b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of X_train: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Type of X_test: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Type of X_train: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Type of X_test: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Type of X_train: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Type of X_test: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Type of X_train: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Type of X_test: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Type of X_train: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Type of X_test: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Average nDCG score across all folds after removing the least important 60 features: 0.6634693900898593\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import ndcg_score\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "\n",
    "\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'ndcg',\n",
    "    'verbosity': -1\n",
    "}\n",
    "\n",
    "data_dir = \"MSLR-WEB10K/\"\n",
    "\n",
    "ndcg_scores = []\n",
    "\n",
    "for fold_num in range(1, 6):\n",
    "    fold_path = os.path.join(data_dir, f\"Fold{fold_num}/\")\n",
    "\n",
    "    # Load the data for the current fold\n",
    "    X_train, y_train, qid_train = load_svmlight_file(os.path.join(fold_path, 'train.txt'), query_id = True)\n",
    "    X_test, y_test, qid_test = load_svmlight_file(os.path.join(fold_path, 'test.txt'), query_id = True)\n",
    "        \n",
    "    print(\"Type of X_train:\", type(X_train))\n",
    "    print(\"Type of X_test:\", type(X_test))\n",
    "    \n",
    "    feature_importance_scores = get_feature_importance(model)\n",
    "\n",
    "    # Filter out the least important feature indices\n",
    "    least_important_indices = np.argsort(feature_importance_scores)[:60]\n",
    "\n",
    "    X_train_csc = X_train.tocsc()\n",
    "    X_test_csc = X_test.tocsc()\n",
    "    \n",
    "    # Remove least important features\n",
    "    X_train_filtered_csc = X_train_csc[:, [i for i in range(X_train_csc.shape[1]) if i not in least_important_indices]]\n",
    "    X_test_filtered_csc = X_test_csc[:, [i for i in range(X_test_csc.shape[1]) if i not in least_important_indices]]\n",
    "    \n",
    "    X_train_filtered = X_train_filtered_csc.tocsr()\n",
    "    X_test_filtered = X_test_filtered_csc.tocsr()\n",
    "\n",
    "    group_train = np.bincount(qid_train)\n",
    "    group_test = np.bincount(qid_test)\n",
    "    \n",
    "    # Create LightGBM datasets \n",
    "    lgb_train = lgb.Dataset(X_train_filtered, y_train, group = group_train)\n",
    "    lgb_test = lgb.Dataset(X_test_filtered, y_test, reference = lgb_train, group = group_test)\n",
    "\n",
    "    # Train LightGBM model\n",
    "    model = lgb.train(params, lgb_train, num_boost_round = 100) \n",
    "    y_pred = model.predict(X_test_filtered)\n",
    "\n",
    "    # Calculate nDCG score for this fold\n",
    "    ndcg = ndcg_score([y_test], [y_pred], k = 10) \n",
    "    ndcg_scores.append(ndcg)\n",
    "\n",
    "# Calculate average nDCG score across all folds\n",
    "average_ndcg = np.mean(ndcg_scores)\n",
    "print(\"Average nDCG score across all folds after removing the least important 60 features:\", average_ndcg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d792702d-8543-4c03-acc8-f1466b8466c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
